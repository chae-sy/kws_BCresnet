{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4523eb1c-2c49-4bcc-b241-79505cf6df40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T06:58:31.116056Z",
     "iopub.status.busy": "2024-09-01T06:58:31.115054Z",
     "iopub.status.idle": "2024-09-01T06:58:45.842613Z",
     "shell.execute_reply": "2024-09-01T06:58:45.841610Z",
     "shell.execute_reply.started": "2024-09-01T06:58:31.116056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c933b02-15e4-4485-9771-ce7fde1d578b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T06:58:48.046392Z",
     "iopub.status.busy": "2024-09-01T06:58:48.045388Z",
     "iopub.status.idle": "2024-09-01T06:58:51.317733Z",
     "shell.execute_reply": "2024-09-01T06:58:51.315729Z",
     "shell.execute_reply.started": "2024-09-01T06:58:48.046392Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44cc87c2-847b-4a0b-b2f0-61316dae8e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T06:58:58.669218Z",
     "iopub.status.busy": "2024-09-01T06:58:58.666219Z",
     "iopub.status.idle": "2024-09-01T06:58:58.680048Z",
     "shell.execute_reply": "2024-09-01T06:58:58.677852Z",
     "shell.execute_reply.started": "2024-09-01T06:58:58.669218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c11bd9f-71ad-4de8-9f6e-ee2772b6778e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T06:59:01.249922Z",
     "iopub.status.busy": "2024-09-01T06:59:01.248922Z",
     "iopub.status.idle": "2024-09-01T06:59:01.280125Z",
     "shell.execute_reply": "2024-09-01T06:59:01.279122Z",
     "shell.execute_reply.started": "2024-09-01T06:59:01.249922Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset 나누기\n",
    "\n",
    "import os\n",
    "import random\n",
    "random.seed(10)\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, dataset_path, keywords, subset, unknown_label, sample_ratio=0.2, noise_label='_background_noise_', num_noise_samples=100):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.keywords = keywords\n",
    "        self.unknown_label = unknown_label\n",
    "        self.noise_label = noise_label\n",
    "        self.num_noise_samples = num_noise_samples\n",
    "        self.all_classes =self.unknown_label+ self.keywords +  [self.noise_label]\n",
    "        self.subset = subset\n",
    "        self.sample_ratio = sample_ratio\n",
    "\n",
    "        self.keywords_to_index=['unknown']+self.keywords+[self.noise_label]\n",
    "        \n",
    "        # Load lists for validation and test datasets\n",
    "        self.validation_list = self._load_file_list(os.path.join(dataset_path, 'validation_list.txt'))\n",
    "        self.testing_list = self._load_file_list(os.path.join(dataset_path, 'testing_list.txt'))\n",
    "\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        self.background_noises = []\n",
    "        unknown_files = []\n",
    "\n",
    "        # Load all audio files and corresponding labels\n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            label = os.path.basename(root)\n",
    "            if label in self.all_classes:\n",
    "                for file in files:\n",
    "                    if file.endswith('.wav'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        file_path = r\"{}\".format(file_path)\n",
    "                        if label == noise_label: #noise append -> (txt에 있으면) unknown에 append + (txt에 있으면) audio_file에 append\n",
    "                            self.background_noises.append(file_path)\n",
    "                        else:\n",
    "                            if label not in self.keywords: #지정 keyword (10개) 중 없는 경우 -> unknown 할당\n",
    "                                label = 'unknown'\n",
    "                                if self._is_in_subset(file_path):\n",
    "                                    unknown_files.append((file_path, label))\n",
    "                            else: \n",
    "                                if self._is_in_subset(file_path):\n",
    "                                    self.audio_files.append(file_path)\n",
    "                                    self.labels.append(label)\n",
    "\n",
    "        # unknown class data를 sample_ratio(default=0.2) 비율로 sampling\n",
    "        if unknown_files:\n",
    "            total_desired_unknowns = int((len(self.audio_files) / (1 - self.sample_ratio)) * self.sample_ratio)\n",
    "            if total_desired_unknowns < len(unknown_files):\n",
    "                unknown_files = random.sample(unknown_files, total_desired_unknowns)\n",
    "\n",
    "        # Add the sampled unknown files to the dataset\n",
    "        for file_path, label in unknown_files:\n",
    "            self.audio_files.append(file_path)\n",
    "            self.labels.append(label)\n",
    "\n",
    "        # Generate random slices of background noise\n",
    "        self.noise_samples = []\n",
    "        for _ in range(num_noise_samples):\n",
    "            noise_path = random.choice(self.background_noises)\n",
    "            waveform, sample_rate = torchaudio.load(noise_path)\n",
    "            max_offset = waveform.size(1) - sample_rate\n",
    "            offset = random.randint(0, max_offset)\n",
    "            noise_slice = waveform[:, offset:offset + sample_rate]\n",
    "            self.noise_samples.append(noise_slice)\n",
    "\n",
    "    def _load_file_list(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            file_list = f.read().splitlines()\n",
    "        return set(file_list)\n",
    "\n",
    "    def _is_in_subset(self, file_path):\n",
    "        relative_path = os.path.relpath(file_path, self.dataset_path)\n",
    "        relative_path = relative_path.replace('\\\\', '/')\n",
    "        if self.subset == 'train':\n",
    "            return relative_path not in self.validation_list and relative_path not in self.testing_list\n",
    "        elif self.subset == 'validation':\n",
    "            return relative_path in self.validation_list\n",
    "        elif self.subset == 'test':\n",
    "            return relative_path in self.testing_list\n",
    "        else:\n",
    "            raise ValueError(\"Subset must be one of ['train', 'validation', 'test']\")\n",
    "\n",
    "    def _pad_waveform(self, waveform, target_length=16000):\n",
    "      current_length = waveform.shape[1]\n",
    "      if current_length < target_length:\n",
    "        pad_amount = target_length - current_length\n",
    "        padding = torch.zeros((waveform.shape[0], pad_amount))  # (channels, pad_amount)\n",
    "        waveform = torch.cat((waveform, padding), dim=1)\n",
    "      return waveform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files) + self.num_noise_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #classes=self.all_classes ## to be fixed\n",
    "        if idx < len(self.audio_files):\n",
    "            file_path = self.audio_files[idx]\n",
    "            label = self.labels[idx]\n",
    "            label_index=self.keywords_to_index.index(label) # label to index\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            waveform=self._pad_waveform(waveform) #padding\n",
    "            filename = os.path.basename(file_path)\n",
    "            speaker_id, utterance_number=tuple(filename.split('_nohash_'))\n",
    "            utterance_number=utterance_number.split('.')[0]\n",
    "        else:\n",
    "            noise_idx = idx - len(self.audio_files)\n",
    "            waveform = self.noise_samples[noise_idx-1]\n",
    "            sample_rate = waveform.shape[1]\n",
    "            label = self.noise_label\n",
    "            label_index=self.keywords_to_index.index(label) # label to index\n",
    "            speaker_id= None\n",
    "            utterance_number=None\n",
    "\n",
    "        return waveform, sample_rate, label_index, speaker_id, utterance_number\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfc0ce96-d963-45d8-b8c7-95c0b5610d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T08:21:07.658854Z",
     "iopub.status.busy": "2024-08-27T08:21:07.656639Z",
     "iopub.status.idle": "2024-08-27T08:21:16.956727Z",
     "shell.execute_reply": "2024-08-27T08:21:16.955430Z",
     "shell.execute_reply.started": "2024-08-27T08:21:07.658854Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "\n",
    "dataset = SPEECHCOMMANDS(root=\"./\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c850f48-a6e5-47c8-aabe-e8954cc22c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T06:59:05.599408Z",
     "iopub.status.busy": "2024-09-01T06:59:05.597365Z",
     "iopub.status.idle": "2024-09-01T06:59:56.888239Z",
     "shell.execute_reply": "2024-09-01T06:59:56.793398Z",
     "shell.execute_reply.started": "2024-09-01T06:59:05.599408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "dataset_path = r'SpeechCommands\\speech_commands_v0.02'\n",
    "unknown_label='backward, bed, bird, cat, dog, eight, five, follow, forward, four, happy, house, learn, marvin, nine, one, seven, sheila, six, three, tree, two, visual, wow, zero'.split(', ')\n",
    "keywords = 'yes, no, up, down, left, right, on, off, stop, go'.split(', ')\n",
    "num_noise_samples = 2000\n",
    "\n",
    "if (len(unknown_label)+len(keywords)==35): #unknown과 keyword가 합쳐서 35개 되는지 확인\n",
    "  # Train dataset\n",
    "  train_set = SpeechCommandsDataset(dataset_path, keywords, unknown_label=unknown_label, subset='train', num_noise_samples=num_noise_samples)\n",
    "\n",
    "  # Validation dataset\n",
    "  #val_set = SpeechCommandsDataset(dataset_path, keywords, unknown_label=unknown_label, subset='validation', num_noise_samples=num_noise_samples)\n",
    "\n",
    "  # Test dataset\n",
    "  test_set = SpeechCommandsDataset(dataset_path, keywords, unknown_label=unknown_label, subset='test', num_noise_samples=num_noise_samples)\n",
    "else: print(\"Error\")\n",
    "\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd04fca-1420-4d5f-ab7b-4b5bc54931c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T07:00:01.419361Z",
     "iopub.status.busy": "2024-09-01T07:00:01.418360Z",
     "iopub.status.idle": "2024-09-01T07:00:01.488701Z",
     "shell.execute_reply": "2024-09-01T07:00:01.487696Z",
     "shell.execute_reply.started": "2024-09-01T07:00:01.419361Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"train_set 개수 : {:}, test_set 개수 : {:}\".format(len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae16993-a9f6-49a3-942d-81ec19060568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T02:16:47.005965Z",
     "iopub.status.busy": "2024-08-30T02:16:47.005965Z",
     "iopub.status.idle": "2024-08-30T02:17:03.243215Z",
     "shell.execute_reply": "2024-08-30T02:17:03.242206Z",
     "shell.execute_reply.started": "2024-08-30T02:16:47.005965Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_list = []\n",
    "for _, _, label, _, _ in test_set:\n",
    "    label_list.append(label)\n",
    "\n",
    "label_counts = Counter(label_list)\n",
    "\n",
    "# 라벨과 개수 오름차순으로 출력\n",
    "for label in sorted(label_counts.keys()):\n",
    "    print(f\"Label: {label}, Count: {label_counts[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48c280-d6b4-4c90-8f3d-ee1c46ad90e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T07:00:05.538468Z",
     "iopub.status.busy": "2024-09-01T07:00:05.537466Z",
     "iopub.status.idle": "2024-09-01T07:00:06.978705Z",
     "shell.execute_reply": "2024-09-01T07:00:06.977700Z",
     "shell.execute_reply.started": "2024-09-01T07:00:05.538468Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.plot(waveform.t().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b9107-ec23-4e87-803c-6e142aaf5515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T07:00:45.404077Z",
     "iopub.status.busy": "2024-09-01T07:00:45.403103Z",
     "iopub.status.idle": "2024-09-01T07:00:47.000268Z",
     "shell.execute_reply": "2024-09-01T07:00:46.999182Z",
     "shell.execute_reply.started": "2024-09-01T07:00:45.404077Z"
    }
   },
   "outputs": [],
   "source": [
    "# spectrogram 변환\n",
    "def sp_transform(waveform):\n",
    "\n",
    "  waveform = waveform.cpu()\n",
    "\n",
    "  spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate = 16000, win_length = 480, n_fft = 480, hop_length = 160, n_mels = 40)\n",
    "  spectrogram = spectrogram_transform(waveform)\n",
    "  epsilon = 1e-6\n",
    "  spectrogram = spectrogram + epsilon\n",
    "  spectrogram = spectrogram.log2()\n",
    "\n",
    "  spectrogram = spectrogram.to(device)\n",
    "  return spectrogram\n",
    "\n",
    "sp = sp_transform(waveform)[0]\n",
    "plt.imshow(sp.cpu().numpy(), aspect='auto')\n",
    "print(\"size of spectrogram: {:}\".format(sp.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28f38e0-8315-42f0-8c8a-8eb7bc722e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T07:01:45.930375Z",
     "iopub.status.busy": "2024-09-01T07:01:45.929375Z",
     "iopub.status.idle": "2024-09-01T07:01:45.946778Z",
     "shell.execute_reply": "2024-09-01T07:01:45.945774Z",
     "shell.execute_reply.started": "2024-09-01T07:01:45.930375Z"
    }
   },
   "outputs": [],
   "source": [
    "#데이터 길이 맞춰주기 위해 padding\n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "# data중 waveform, label(index로 된)를 각각 tensor, target에 추가\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.tensor(targets)\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "# train, test 위한 data batch_size로 나눔\n",
    "batch_size = 128\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d84d37-d216-4438-9312-c8bc8ef8b04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T07:01:49.673228Z",
     "iopub.status.busy": "2024-09-01T07:01:49.672256Z",
     "iopub.status.idle": "2024-09-01T07:01:49.734365Z",
     "shell.execute_reply": "2024-09-01T07:01:49.732362Z",
     "shell.execute_reply.started": "2024-09-01T07:01:49.673228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Qualcomm Technologies, Inc.\n",
    "# All Rights Reserved.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_plane,\n",
    "        out_plane,\n",
    "        idx,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        groups=1,\n",
    "        use_dilation=False,\n",
    "        activation=True,\n",
    "        swish=False,\n",
    "        BN=True,\n",
    "        ssn=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        def get_padding(kernel_size, use_dilation):\n",
    "            rate = 1  # dilation rate\n",
    "            padding_len = (kernel_size - 1) // 2\n",
    "            if use_dilation and kernel_size > 1:\n",
    "                rate = int(2**self.idx)\n",
    "                padding_len = rate * padding_len\n",
    "            return padding_len, rate\n",
    "\n",
    "        self.idx = idx\n",
    "\n",
    "        # padding and dilation rate\n",
    "        if isinstance(kernel_size, (list, tuple)):\n",
    "            padding = []\n",
    "            rate = []\n",
    "            for k_size in kernel_size:\n",
    "                temp_padding, temp_rate = get_padding(k_size, use_dilation)\n",
    "                rate.append(temp_rate)\n",
    "                padding.append(temp_padding)\n",
    "        else:\n",
    "            padding, rate = get_padding(kernel_size, use_dilation)\n",
    "\n",
    "        # convbnrelu block\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_plane, out_plane, kernel_size, stride, padding, rate, groups, bias=False)\n",
    "        )\n",
    "        if ssn:\n",
    "            layers.append(SubSpectralNorm(out_plane, 5))\n",
    "        elif BN:\n",
    "            layers.append(nn.BatchNorm2d(out_plane))\n",
    "        if swish:\n",
    "            layers.append(nn.SiLU(True))\n",
    "        elif activation:\n",
    "            layers.append(nn.ReLU6(True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class BCResBlock(nn.Module):\n",
    "    def __init__(self, in_plane, out_plane, idx, stride):\n",
    "        super().__init__()\n",
    "        self.transition_block = in_plane != out_plane\n",
    "        kernel_size = (3, 3)\n",
    "\n",
    "        # 2D part (f2)\n",
    "        layers = []\n",
    "        if self.transition_block:\n",
    "            layers.append(ConvBNReLU(in_plane, out_plane, idx, 1, 1))\n",
    "            in_plane = out_plane\n",
    "        layers.append(\n",
    "            ConvBNReLU(\n",
    "                in_plane,\n",
    "                out_plane,\n",
    "                idx,\n",
    "                (kernel_size[0], 1),\n",
    "                (stride[0], 1),\n",
    "                groups=in_plane,\n",
    "                ssn=False, # modified\n",
    "                activation=False,\n",
    "            )\n",
    "        )\n",
    "        self.f2 = nn.Sequential(*layers)\n",
    "        self.avg_gpool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        # 1D part (f1)\n",
    "        self.f1 = nn.Sequential(\n",
    "            ConvBNReLU(\n",
    "                out_plane,\n",
    "                out_plane,\n",
    "                idx,\n",
    "                (1, kernel_size[1]),\n",
    "                (1, stride[1]),\n",
    "                groups=out_plane,\n",
    "                swish=False,\n",
    "                use_dilation=True,\n",
    "            ),\n",
    "            nn.Conv2d(out_plane, out_plane, 1, bias=False),\n",
    "            nn.Dropout2d(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2D part\n",
    "        shortcut = x\n",
    "        x = self.f2(x)\n",
    "        aux_2d_res = x\n",
    "        x = self.avg_gpool(x)\n",
    "\n",
    "        # 1D part\n",
    "        x = self.f1(x)\n",
    "        x = x + aux_2d_res\n",
    "        if not self.transition_block:\n",
    "            x = x + shortcut\n",
    "        x = F.relu6(x, True)\n",
    "        return x\n",
    "\n",
    "\n",
    "def BCBlockStage(num_layers, last_channel, cur_channel, idx, use_stride):\n",
    "    stage = nn.ModuleList()\n",
    "    channels = [last_channel] + [cur_channel] * num_layers\n",
    "    for i in range(num_layers):\n",
    "        stride = (2, 1) if use_stride and i == 0 else (1, 1)\n",
    "        stage.append(BCResBlock(channels[i], channels[i + 1], idx, stride))\n",
    "    return stage\n",
    "\n",
    "\n",
    "class BCResNets(nn.Module):\n",
    "    def __init__(self, base_c, num_classes=12):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.n = [1, 1, 1]  # identical modules repeated n times\n",
    "        self.c = [\n",
    "            int(base_c * 1.5),\n",
    "            base_c,\n",
    "            int(base_c * 1.5),\n",
    "            base_c * 2,\n",
    "            #int(base_c * 2.5),\n",
    "            base_c * 4,\n",
    "        ]  # num channels\n",
    "        self.s = [1, 2]  # stage using stride\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        # Head: (Conv-BN-ReLU)\n",
    "        self.cnn_head = nn.Sequential(\n",
    "            nn.Conv2d(1, self.c[0], 5, (2, 1), 2, bias=False),\n",
    "            nn.BatchNorm2d(self.c[0]),\n",
    "            nn.ReLU6(True),\n",
    "        )\n",
    "        # Body: BC-ResBlocks\n",
    "        self.BCBlocks = nn.ModuleList([])\n",
    "        for idx, n in enumerate(self.n):\n",
    "            use_stride = idx in self.s\n",
    "            self.BCBlocks.append(BCBlockStage(n, self.c[idx], self.c[idx + 1], idx, use_stride))\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.Conv2d(\n",
    "            #    self.c[-2], self.c[-2], (5, 5), bias=False, groups=self.c[-2], padding=(0, 2)\n",
    "            #),\n",
    "            #nn.Conv2d(self.c[-2], self.c[-1], 1, bias=False),\n",
    "            #nn.BatchNorm2d(self.c[-1]),\n",
    "            #nn.ReLU6(True),\n",
    "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            #nn.Conv2d(self.c[-1], self.num_classes, 1),\n",
    "\n",
    "            #nn.Conv2d(self.c[-2], self.c[-1], kernel_size=1),\n",
    "            nn.BatchNorm2d(self.c[-2]),\n",
    "            nn.Conv2d(self.c[-2], self.num_classes, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_head(x)\n",
    "        for i, num_modules in enumerate(self.n):\n",
    "            for j in range(num_modules):\n",
    "                x = self.BCBlocks[i][j](x)\n",
    "        x = self.classifier(x)\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        return x\n",
    "\n",
    "class SubSpectralNorm(nn.Module):\n",
    "    def __init__(self, num_features, spec_groups=16, affine=\"Sub\", batch=True, dim=2):\n",
    "        super().__init__()\n",
    "        self.spec_groups = spec_groups\n",
    "        self.affine_all = False\n",
    "        affine_norm = False\n",
    "        if (\n",
    "            affine == \"Sub\"\n",
    "        ):  # affine transform for each sub group. use affine of torch implementation\n",
    "            affine_norm = True\n",
    "        elif affine == \"All\":\n",
    "            self.affine_all = True\n",
    "            self.weight = nn.Parameter(torch.ones((1, num_features, 1, 1)))\n",
    "            self.bias = nn.Parameter(torch.zeros((1, num_features, 1, 1)))\n",
    "        if batch:\n",
    "            self.ssnorm = nn.BatchNorm2d(num_features * spec_groups, affine=affine_norm)\n",
    "        else:\n",
    "            self.ssnorm = nn.InstanceNorm2d(num_features * spec_groups, affine=affine_norm)\n",
    "        self.sub_dim = dim\n",
    "\n",
    "    def forward(self, x):  # when dim h is frequency dimension\n",
    "        if self.sub_dim in (3, -1):\n",
    "            x = x.transpose(2, 3)\n",
    "            x = x.contiguous()\n",
    "        b, c, h, w = x.size()\n",
    "        assert h % self.spec_groups == 0\n",
    "        x = x.view(b, c * self.spec_groups, h // self.spec_groups, w)\n",
    "        x = self.ssnorm(x)\n",
    "        x = x.view(b, c, h, w)\n",
    "        if self.affine_all:\n",
    "            x = x * self.weight + self.bias\n",
    "        if self.sub_dim in (3, -1):\n",
    "            x = x.transpose(2, 3)\n",
    "            x = x.contiguous()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd94bd-627e-4f10-a09b-b0be5cadfd65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T02:43:33.262024Z",
     "iopub.status.busy": "2024-08-30T02:43:33.261050Z",
     "iopub.status.idle": "2024-08-30T02:43:34.478292Z",
     "shell.execute_reply": "2024-08-30T02:43:34.477287Z",
     "shell.execute_reply.started": "2024-08-30T02:43:33.262024Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model=BCResNets(base_c=8)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(f\"Number of parameters: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f576ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "input = torch.randn(1, 1, 40, 101).to(device)\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3dee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "total_epoch = 100\n",
    "warmup_epoch = 5\n",
    "init_lr = 1e-1\n",
    "lr_lower_limit = 0\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0, weight_decay=1e-3, momentum=0.9)\n",
    "n_step_warmup = len(train_loader) * warmup_epoch\n",
    "total_iter = len(train_loader) * total_epoch\n",
    "iterations = 0\n",
    "\n",
    "# train\n",
    "for epoch in range(total_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sample in tqdm(train_loader, desc=\"epoch %d, iters\" % (epoch + 1)):\n",
    "        # lr cos schedule\n",
    "        iterations += 1\n",
    "        if iterations < n_step_warmup:\n",
    "            lr = init_lr * iterations / n_step_warmup\n",
    "        else:\n",
    "            lr = lr_lower_limit + 0.5 * (init_lr - lr_lower_limit) * (\n",
    "                1\n",
    "                + np.cos(\n",
    "                    np.pi * (iterations - n_step_warmup) / (total_iter - n_step_warmup)\n",
    "                )\n",
    "            )\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        inputs, labels = sample\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        inputs = sp_transform(inputs)\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Calculate and print epoch loss and accuracy\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{total_epoch} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d48d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(dataset, loader, augment):\n",
    "        \"\"\"\n",
    "        Tests the model on a given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            dataset (Dataset): The dataset to test the model on.\n",
    "            loader (DataLoader): The data loader to use for batching the data.\n",
    "            augment (bool): Flag indicating whether to use data augmentation during testing.\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy of the model on the given dataset.\n",
    "        \"\"\"\n",
    "        true_count = 0.0\n",
    "        num_testdata = float(len(dataset))\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = sp_transform(inputs)\n",
    "            outputs = model(inputs)\n",
    "            prediction = torch.argmax(outputs, dim=-1)\n",
    "            true_count += torch.sum(prediction == labels).detach().cpu().numpy()\n",
    "            print('answer: {:} predicted: {:}'.format(labels, prediction))\n",
    "        acc = true_count / num_testdata * 100.0  # percentage\n",
    "        return acc\n",
    "\n",
    "\n",
    "test_acc = Test(test_set, test_loader, augment=False)  # official testset\n",
    "print(\"test acc: %.3f\" % (test_acc))\n",
    "print(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(dataset, loader, augment):\n",
    "        \"\"\"\n",
    "        Tests the model on a given dataset.\n",
    "\n",
    "        Parameters:\n",
    "            dataset (Dataset): The dataset to test the model on.\n",
    "            loader (DataLoader): The data loader to use for batching the data.\n",
    "            augment (bool): Flag indicating whether to use data augmentation during testing.\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy of the model on the given dataset.\n",
    "        \"\"\"\n",
    "        true_count = 0.0\n",
    "        num_testdata = float(len(dataset))\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = sp_transform(inputs)\n",
    "            outputs = model(inputs)\n",
    "            prediction = torch.argmax(outputs, dim=-1)\n",
    "            true_count += torch.sum(prediction == labels).detach().cpu().numpy()\n",
    "        acc = true_count / num_testdata * 100.0  # percentage\n",
    "        return acc\n",
    "  \n",
    "\n",
    "test_acc = Test(dataset=test_set, loader=test_loader, augment=False)  # official testset\n",
    "print(\"test acc: %.3f\" % (test_acc))\n",
    "print(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33288bd-5b94-4a8b-ba1c-e5beaed53662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T08:43:02.552338Z",
     "iopub.status.busy": "2024-08-27T08:43:02.552338Z",
     "iopub.status.idle": "2024-08-27T08:43:02.929210Z",
     "shell.execute_reply": "2024-08-27T08:43:02.929210Z",
     "shell.execute_reply.started": "2024-08-27T08:43:02.552338Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses);\n",
    "plt.title(\"training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b77bf3-829d-4eeb-b74d-9dc127cd52bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T08:43:11.172210Z",
     "iopub.status.busy": "2024-08-27T08:43:11.172210Z",
     "iopub.status.idle": "2024-08-27T08:43:11.972744Z",
     "shell.execute_reply": "2024-08-27T08:43:11.971735Z",
     "shell.execute_reply.started": "2024-08-27T08:43:11.172210Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = sp_transform(tensor)\n",
    "\n",
    "    # 입력 텐서가 2D 또는 3D일 때 올바른 차원으로 확장\n",
    "    if tensor.dim() == 2:  # 예: (height, width)인 경우\n",
    "        tensor = tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, height, width)로 확장\n",
    "    elif tensor.dim() == 3:  # 예: (channels, height, width)인 경우\n",
    "        tensor = tensor.unsqueeze(0)  # (1, channels, height, width)로 확장\n",
    "\n",
    "    tensor = model(tensor)\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = tensor.squeeze()\n",
    "    return tensor\n",
    "\n",
    "# 나머지 코드는 동일하게 유지\n",
    "waveform, sample_rate, utterance, *_ = train_set[100]\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "print(f\"Expected: {utterance}. Predicted: {predict(waveform)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8ee69-302d-4a8f-859c-d47affb493d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T08:55:29.680962Z",
     "iopub.status.busy": "2024-08-27T08:55:29.679965Z",
     "iopub.status.idle": "2024-08-27T08:56:52.870719Z",
     "shell.execute_reply": "2024-08-27T08:56:52.867626Z",
     "shell.execute_reply.started": "2024-08-27T08:55:29.680962Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 실제 라벨과 예측된 라벨을 저장할 리스트 초기화\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# 테스트 셋을 순회하며 예측 수행\n",
    "for i, (waveform, sample_rate, utterance, *_) in enumerate(test_set):\n",
    "    output = predict(waveform)\n",
    "    true_labels.append(utterance)\n",
    "    predicted_labels.append(output)\n",
    "\n",
    "# 라벨 리스트 정의 (unknown 라벨도 포함)\n",
    "labels_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "cm = confusion_matrix(true_labels, predicted_labels, labels=labels_list)\n",
    "\n",
    "# 각 라벨에 대한 정확도 계산\n",
    "label_accuracies = {}\n",
    "for i, label in enumerate(labels_list):\n",
    "    true_positive = cm[i, i]\n",
    "    total_samples = cm[i, :].sum()\n",
    "    accuracy = true_positive / total_samples if total_samples > 0 else 0\n",
    "    label_accuracies[label] = accuracy\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_list, yticklabels=labels_list)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# # 혼동 행렬을 텍스트 파일로 저장\n",
    "# output_matrix_path = '/content/confusion_matrix.txt'\n",
    "# np.savetxt(output_matrix_path, cm, fmt='%d', delimiter=',', header=','.join(labels_list), comments='')\n",
    "# print(f\"Confusion matrix saved to {output_matrix_path}\")\n",
    "\n",
    "# 각 라벨에 대한 정확도 출력\n",
    "for label, accuracy in label_accuracies.items():\n",
    "    print(f\"Accuracy for label '{label}': {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
